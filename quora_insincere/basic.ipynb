{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandasql import sqldf\n",
    "import boto3\n",
    "import json\n",
    "import os\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from keras import models, layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# from sklearn.preprocessing import Normalizer\n",
    "# %run '../extra_fns.ipynb'\n",
    "# from sklearn.cluster import KMeans\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import xgboost\n",
    "# from sklearn.metrics import mean_absolute_error as mae\n",
    "# from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../config.json') as json_data:\n",
    "    config = json.load(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=config['boto']['aws_access_key_id'],\n",
    "    aws_secret_access_key=config['boto']['aws_secret_access_key']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 17] File exists: 'trained_models'\n"
     ]
    }
   ],
   "source": [
    "model_dir = 'trained_models'\n",
    "try:\n",
    "    os.makedirs(model_dir)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "    \n",
    "train_file = data_dir + '/train.csv'\n",
    "test_file = data_dir + '/test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3.upload_file(train_file, config['boto']['buckets']['kaggle'], train_file)\n",
    "# s3.upload_file(src_file_cleaned, boto_config['buckets']['kaggle'], src_file_cleaned)\n",
    "\n",
    "# s3.download_file(boto_config['buckets']['kaggle'], src_file, src_file)\n",
    "# s3.download_file(boto_config['buckets']['kaggle'], src_file_cleaned, src_file_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(train_file)\n",
    "test = pd.read_csv(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of positive class: 6.187017751787352\n"
     ]
    }
   ],
   "source": [
    "print('% of positive class: {0}'.format(train.target.sum()/train.shape[0]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['question_length'] = train.question_text.str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1.306122e+06\n",
       "mean     7.067884e+01\n",
       "std      3.878428e+01\n",
       "min      1.000000e+00\n",
       "25%      4.500000e+01\n",
       "50%      6.000000e+01\n",
       "75%      8.500000e+01\n",
       "max      1.017000e+03\n",
       "Name: question_length, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.question_length.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_maxlen = 100\n",
    "train = train[train.question_length<=seq_maxlen]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_val, y_train, y_val = train_test_split(train.question_text, train.target, \n",
    "#                                                   test_size=0.1, \n",
    "#                                                   random_state=1, \n",
    "#                                                   stratify=train.target)\n",
    "\n",
    "X_train = train.question_text\n",
    "y_train = train.target\n",
    "X_test = test.question_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequentialize words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tk.texts_to_sequences(X_train)\n",
    "# X_val = tk.texts_to_sequences(X_val)\n",
    "X_test = tk.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train, maxlen=seq_maxlen)\n",
    "# X_val = pad_sequences(X_val, maxlen=seq_maxlen)\n",
    "X_test = pad_sequences(X_test, maxlen=seq_maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Embedding())\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "model.summmary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10, batch_size=512, validation_split=0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train['distance'] = train.apply(lambda row : int(distance(\n",
    "#     (row['pickup_latitude'], row['pickup_longitude']),\n",
    "#     (row['dropoff_latitude'], row['dropoff_longitude'])).m), axis=1)\n",
    "\n",
    "# train.pickup_datetime = pd.to_datetime(train.pickup_datetime)\n",
    "\n",
    "# train['pickup_monthday'] = train.pickup_datetime.dt.day\n",
    "# train['pickup_weekday'] = train.pickup_datetime.dt.weekday\n",
    "# train['pickup_hour'] = train.pickup_datetime.dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test['distance'] = test.apply(lambda row : int(distance(\n",
    "#     (row['pickup_latitude'], row['pickup_longitude']),\n",
    "#     (row['dropoff_latitude'], row['dropoff_longitude'])).m), axis=1)\n",
    "\n",
    "# test.pickup_datetime = pd.to_datetime(test.pickup_datetime)\n",
    "\n",
    "# test['pickup_monthday'] = test.pickup_datetime.dt.day\n",
    "# test['pickup_weekday'] = test.pickup_datetime.dt.weekday\n",
    "# test['pickup_hour'] = test.pickup_datetime.dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train.to_csv('./data/train-clean.csv')\n",
    "# test.to_csv('./data/test-clean.csv')\n",
    "\n",
    "train = pd.read_csv('./data/train-clean.csv')\n",
    "test = pd.read_csv('./data/test-clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[(train.trip_duration<3600) & (train.distance<30000) & (train.trip_duration>180)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='trip_duration', y='distance', data=train.sample(10000))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['avg_speed'] = (train.distance*3600)/(train.trip_duration*1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[(train.avg_speed<70) & (train.avg_speed>8)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clusterize Geopoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distortions = []\n",
    "for i in range(1, 11):\n",
    "    km = KMeans(n_clusters=i,\n",
    "                init='k-means++', \n",
    "                n_init=10,\n",
    "                max_iter=300,\n",
    "                random_state=1)\n",
    "    km.fit(train[['pickup_latitude', 'pickup_longitude']])\n",
    "    distortions.append(km.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1,11), distortions, marker='o')\n",
    "plt.xlabel('clusters')\n",
    "plt.ylabel('distortion')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 5\n",
    "km = KMeans(n_clusters=n_clusters,\n",
    "                init='k-means++', \n",
    "                n_init=10,\n",
    "                max_iter=300,\n",
    "                random_state=1)\n",
    "km.fit(train[['pickup_latitude', 'pickup_longitude']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['pickup_geocluster'] = km.predict(train[['pickup_latitude', 'pickup_longitude']])\n",
    "train['dropoff_geocluster'] = km.predict(train[['dropoff_latitude', 'dropoff_longitude']])\n",
    "\n",
    "test['pickup_geocluster'] = km.predict(test[['pickup_latitude', 'pickup_longitude']])\n",
    "test['dropoff_geocluster'] = km.predict(test[['dropoff_latitude', 'dropoff_longitude']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = list(range(n_clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.join(pd.get_dummies(train.pickup_geocluster, prefix='pickup_geocluster'))\n",
    "train = train.join(pd.get_dummies(train.dropoff_geocluster, prefix='dropoff_geocluster'))\n",
    "test = test.join(pd.get_dummies(test.pickup_geocluster, prefix='pickup_geocluster'))\n",
    "test = test.join(pd.get_dummies(test.dropoff_geocluster, prefix='dropoff_geocluster'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickup_clusters = ['pickup_geocluster_{0}'.format(i) for i in range(n_clusters)]\n",
    "dropoff_clusters = ['dropoff_geocluster_{0}'.format(i) for i in range(n_clusters)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['passenger_count', 'distance', 'pickup_monthday', 'pickup_weekday', 'pickup_hour'] + pickup_clusters + dropoff_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train = np.array(train.trip_duration)\n",
    "X_train = train.loc[:, cols]\n",
    "\n",
    "X_test = test.loc[:, cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_mean_absolute_error', patience=1),\n",
    "    ModelCheckpoint(filepath=model_dir + '/basic_model.h5', monitor='val_loss', save_best_only=True)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nml = Normalizer()\n",
    "\n",
    "X_train = nml.fit_transform(X_train)\n",
    "X_val = nml.transform(X_val)\n",
    "X_test = nml.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nn = models.Sequential()\n",
    "nn.add(layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "nn.add(layers.Dense(32, activation='relu'))\n",
    "nn.add(layers.Dropout(0.2))\n",
    "nn.add(layers.Dense(16, activation='relu'))\n",
    "nn.add(layers.Dropout(0.2))\n",
    "nn.add(layers.Dense(1))\n",
    "\n",
    "nn.compile(loss='mse', optimizer='rmsprop', metrics=['mae'])\n",
    "\n",
    "nn.summary()\n",
    "\n",
    "history = nn.fit(X_train, \n",
    "                    y_train, \n",
    "                    epochs=20, \n",
    "                    batch_size=512,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = history.history\n",
    "epochs = history.epoch\n",
    "\n",
    "print('Min Val Absolute Error {0} on Epoch {1}'.format(np.min(results['val_mean_absolute_error']), np.argmin(results['val_mean_absolute_error'])))\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(18,6))\n",
    "\n",
    "ax1.plot(epochs, results['mean_absolute_error'], label='train')\n",
    "ax1.plot(epochs, results['val_mean_absolute_error'], label='val')\n",
    "ax1.set_title('Accuracy')\n",
    "# ax1.set_ylim([0, 1])\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(epochs, results['loss'], label='train')\n",
    "ax2.plot(epochs, results['val_loss'], label='val')\n",
    "ax2.set_title('Loss')\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg = xgboost.XGBRegressor(nthread=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_predictions = xg.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae(y_val, xg_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_predictions = nn.predict(X_test)\n",
    "xg_predictions = xg.predict(X_test)\n",
    "xg_predictions = xg_predictions.reshape((nn_predictions.shape[0],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = nn_predictions*0.5+xg_predictions*0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs = pd.DataFrame()\n",
    "subs['id'] = test.id\n",
    "subs['trip_duration'] = predictions\n",
    "\n",
    "subs.to_csv(data_dir + '/subs.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
